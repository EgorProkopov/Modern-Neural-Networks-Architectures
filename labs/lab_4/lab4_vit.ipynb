{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках этой лабораторной работы вам предстоит реализовать ViT с нуля. Не обязательно добиваться высокого качества обучения, главное, чтобы запускался и был лучше random.\n",
    "\n",
    "Датасет: https://www.kaggle.com/datasets/gpiosenka/headgear-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size: int = 16,  # размер патча в пикселях\n",
    "        in_channels: int = 3,  # число каналов у входного изображения\n",
    "        embed_dim: int = 64,  # размерность вектора, в который будет преобразован патч\n",
    "\t):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        self._embedder = ...\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProjection(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224, # размер исходного изображение\n",
    "        patch_size: int = 16,\n",
    "        in_channels: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embedder = ...\n",
    "\n",
    "        # эмбеддинги позиции - обучаемый параметр\n",
    "        self.pos_embeddings = ...\n",
    "        \n",
    "        # токен класса\n",
    "        self.cls_token = ...\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        # получаем патчи изображения\n",
    "        # добавляем к эмбеддингам токен класса\n",
    "        # складываем с матрицей позиционных эмбеддингов\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 768,\n",
    "        qkv_dim: int = 64,\n",
    "        dropout_rate: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Нужно создать слои для Q, K, V, не забыть про нормализацию\n",
    "        # на корень из qkv_dim и про дропаут.\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        # считаем аттеншн\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_heads: int = 12,\n",
    "        embed_dim: int = 768,\n",
    "        qkv_dim: int = 64,\n",
    "        dropout_rate: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attns = ...\n",
    "\n",
    "        # внутри блоков трансформера размерность тензора не меняется\n",
    "        # поэтому надо не забыть соединить тензоры после нескольких голов аттеншна\n",
    "        self.projection = ...\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        # считаем аттеншн для нескольких голов\n",
    "        # объединяем\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 768,\n",
    "        mlp_hidden_size: int = 3072,\n",
    "        dropout_rate: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # обычный многослойный перцептрон, но с ориентиром на статью\n",
    "        # не забываем про GELU\n",
    "        self.mlp = ...\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_heads: int = 12,\n",
    "            qkv_dim: int = 64,\n",
    "            embed_dim: int = 768,\n",
    "            mlp_hidden_size: int = 3072,\n",
    "            attn_p: float = 0.1,\n",
    "            mlp_p: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, tensor):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        in_channels: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        qkv_dim: int = 64,\n",
    "        mlp_hidden_size: int = 3072,\n",
    "        n_layers: int = 12,\n",
    "        n_heads: int = 12,\n",
    "        n_classes: int = 1_000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # энкодер \n",
    "        self.encoder = ...\n",
    "\n",
    "        # и классификационная голова\n",
    "        self.classifier = ...\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itmo_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
