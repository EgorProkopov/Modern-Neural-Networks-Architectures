{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/itmo_dl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from torchvision.models import *\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x116742e90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Объединяем две модальности: ViT и GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательные функции и классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    transform = Compose([\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(\n",
    "            mean=(0.485, 0.456, 0.406),\n",
    "            std=(0.229, 0.224, 0.225),\n",
    "        )\n",
    "    ])\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = transform(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, loss_fn, num_epochs=5, device=\"mps\"):\n",
    "    model.vit.to(device)\n",
    "    model.gpt2.to(device)\n",
    "    model.adapter.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        tqdm_iterator = tqdm(dataloader)\n",
    "        tqdm_iterator.set_description(f\"Epoch {epoch+1}/{num_epochs} \")\n",
    "        for batch in tqdm_iterator:\n",
    "            images, captions = batch\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            outputs = model(images, captions)\n",
    "            # убираем последний токен из предсказаний\n",
    "            logits = outputs.logits[:, :- 1, :]\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(\n",
    "                logits.contiguous().view(-1, logits.size(-1)),\n",
    "                captions.contiguous().view(-1)\n",
    "            )\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            tqdm_iterator.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer,\n",
    "            data_dir=\"./flickr8k\",\n",
    "            captions_file=\"captions.txt\",\n",
    "            max_len=50,\n",
    "            ):\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_dir = os.path.join(data_dir, \"Images\")\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.images = []\n",
    "        self.captions = []\n",
    "        captions_file = pd.read_csv(os.path.join(data_dir, captions_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, caption = self.images[idx], self.captions[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        # Load and preprocess image\n",
    "        image = preprocess_image(image_path)\n",
    "        input_ids = self.tokenizer(\n",
    "            caption,\n",
    "            return_tensors='pt',\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len\n",
    "            ).input_ids.squeeze(0)\n",
    "        return image, input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './exotic-shorthair.jpg'\n",
    "image = preprocess_image(image_path).unsqueeze(0)\n",
    "text = 'Describe this image: '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кодировщик и декодировщик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем просто объединить ViT и GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # определим vit\n",
    "        self.vit = vit_b_32(weights=ViT_B_32_Weights.IMAGENET1K_V1)\n",
    "        # уберем голову классификации для получения эмбеддингов\n",
    "        self.vit.heads = nn.Identity()\n",
    "\n",
    "        # определим токенизатор\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # определим ллм\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.max_new = 50\n",
    "\n",
    "        # определим адаптер\n",
    "        self.adapter = nn.Identity()\n",
    "\n",
    "    def forward(self, image, input_ids):\n",
    "        # получим эмбеддинги изображения\n",
    "        visual_embeddings = self.vit(image) # [batch, vit.embed_dim]\n",
    "        # пропустим через адаптер\n",
    "        visual_embeddings = self.adapter(visual_embeddings)  # [batch, gpt.embed_dim]\n",
    "        # токены текста преобразуем их в эмбеддинги\n",
    "        text_embeddings = self.gpt2.transformer.wte(input_ids) \n",
    "        combined_embeddings = torch.cat([\n",
    "            visual_embeddings.unsqueeze(1),  # [batch, 1, emb_dim]\n",
    "            text_embeddings,  # [batch, seq_len, emb_dim]\n",
    "            ], dim=1)\n",
    "\n",
    "        # маска внимания необходима для генерации выхода gpt\n",
    "        attn_mask = torch.cat([\n",
    "            torch.ones((image.shape[0], 1), device=image.device), # и не забываем про изображение\n",
    "            (input_ids != self.tokenizer.pad_token_id).long(),  # обращаем внимание на токены слов\n",
    "        ], dim=1)\n",
    "\n",
    "        # получим выход\n",
    "        outputs = self.gpt2(\n",
    "            inputs_embeds=combined_embeddings,\n",
    "            attention_mask=attn_mask,\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def generate_caption(self, image, text):\n",
    "        # получим эмбеддинги изображения\n",
    "        visual_embeddings = self.vit(image) # [batch, vit.embed_dim]\n",
    "        # пропустим через адаптер\n",
    "        visual_embeddings = self.adapter(visual_embeddings)  # [batch, gpt.embed_dim]\n",
    "\n",
    "        # получим токены текста и преобразуем их в эмбеддинги\n",
    "        input_ids = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "        text_embeddings = self.gpt2.transformer.wte(input_ids) \n",
    "        combined_embeddings = torch.cat([\n",
    "            visual_embeddings.unsqueeze(1),  # [batch, 1, emb_dim]\n",
    "            text_embeddings,  # [batch, seq_len, emb_dim]\n",
    "            ], dim=1)\n",
    "\n",
    "        # маска внимания необходима для генерации выхода gpt\n",
    "        attn_mask = torch.cat([\n",
    "            torch.ones((image.shape[0], 1), device=image.device), # и не забываем про изображение\n",
    "            (input_ids != self.tokenizer.pad_token_id).long(),  # обращаем внимание на токены слов\n",
    "        ], dim=1)\n",
    "\n",
    "        # получим выход\n",
    "        outputs = self.gpt2.generate(\n",
    "            inputs_embeds=combined_embeddings,\n",
    "            attention_mask=attn_mask,\n",
    "            max_new_tokens=self.max_new,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def freeze_vit(self):\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def freeze_gpt(self):\n",
    "        for param in self.gpt2.parameters():\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm = VLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption:  \"The first time I saw the first of the new, new, new, new, new, new, new, new, new, new, new, new, new, new, new, new, new, new, new, new\n"
     ]
    }
   ],
   "source": [
    "caption = vlm.generate_caption(image, text)\n",
    "print(\"Generated caption:\", caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кодировщик, адаптер и декодировщик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейный адаптер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем \"подружить\" ViT и GPT-2. Для этого попробуем дообучить дополнительный слой, который будет проецировать эмбеддинг изображения в пространство, понятное языковой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm_lin = VLM()\n",
    "\n",
    "vlm_lin.adapter = nn.Sequential(\n",
    "    nn.Linear(vlm_lin.vit.hidden_dim, vlm_lin.vit.hidden_dim),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(vlm_lin.vit.hidden_dim, vlm_lin.vit.hidden_dim),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(vlm_lin.vit.hidden_dim, vlm_lin.gpt2.config.n_embd),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption:  \"I was in the middle of a long day in the middle of the night when I saw a man in a white robe walking down the street. I was so shocked that I thought he was a man. I thought he was a man.\n"
     ]
    }
   ],
   "source": [
    "caption = vlm_lin.generate_caption(image, text)\n",
    "print(\"Generated caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Flickr8kDataset(\n",
    "    tokenizer=vlm_lin.tokenizer,\n",
    "    )\n",
    "optimizer = torch.optim.Adam(vlm_lin.adapter.parameters(), lr=1e-4)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "loss = nn.CrossEntropyLoss(ignore_index=vlm_lin.tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 :   0%|          | 0/5057 [00:00<?, ?it/s]d:\\vscode\\drivers-modelling\\venv\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Epoch 1/5 : 100%|██████████| 5057/5057 [07:29<00:00, 11.25it/s, loss=5.91]\n",
      "Epoch 2/5 : 100%|██████████| 5057/5057 [07:36<00:00, 11.09it/s, loss=5.15]\n",
      "Epoch 3/5 : 100%|██████████| 5057/5057 [07:40<00:00, 10.98it/s, loss=5.69]\n",
      "Epoch 4/5 : 100%|██████████| 5057/5057 [07:40<00:00, 10.98it/s, loss=4.85]\n",
      "Epoch 5/5 : 100%|██████████| 5057/5057 [07:43<00:00, 10.91it/s, loss=5.31]\n"
     ]
    }
   ],
   "source": [
    "vlm_lin.freeze_gpt()\n",
    "vlm_lin.freeze_vit()\n",
    "\n",
    "train(\n",
    "    vlm_lin,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    loss,\n",
    "    num_epochs=5,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/01/ljq5wf3159543w3273r2npnr0000gn/T/ipykernel_37762/3518462215.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vlm_lin.load_state_dict(torch.load('./vlm_lin.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vlm_lin.load_state_dict(torch.load('./vlm_lin.pt', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption:  a man on a bicycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "vlm_lin.to(\"cpu\")\n",
    "caption = vlm_lin.generate_caption(image, text)\n",
    "print(\"Generated caption:\", caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Адаптер с Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте немного усложним адаптер, добавив туда Attention. По сути, сделаем мини-блок трансформера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionAdapter(nn.Module):\n",
    "    def __init__(self, vit_dim, gpt_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Linear(vit_dim, gpt_dim)\n",
    "        # в торче уже есть написанный за вас mha\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=gpt_dim, num_heads=num_heads, dropout=dropout)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(gpt_dim, gpt_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gpt_dim * 4, gpt_dim)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(gpt_dim)\n",
    "        self.norm2 = nn.LayerNorm(gpt_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, visual_embeddings, text_embeddings, attention_mask=None):\n",
    "        visual_embeddings = self.proj(visual_embeddings)\n",
    "\n",
    "        # переставляем местами размерности для подачи в слой внимания\n",
    "        visual_embeddings = visual_embeddings.permute(1, 0, 2)  # [1, batch_size, gpt_dim]\n",
    "        text_embeddings = text_embeddings.permute(1, 0, 2)  # [seq_len, batch_size, gpt_dim]\n",
    "\n",
    "        # используем cross-attention: в качестве query подаем эмбеддинги текста,\n",
    "        # в качестве key и value подаем эмбеддинги изображения\n",
    "        attn_output, _ = self.cross_attention(\n",
    "            query=text_embeddings,  # [seq_len, batch_size, gpt_dim]\n",
    "            key=visual_embeddings,    # [1, batch_size, gpt_dim]\n",
    "            value=visual_embeddings,  # [1, batch_size, gpt_dim]\n",
    "            key_padding_mask=None\n",
    "        )\n",
    "        text_embeddings = self.norm1(text_embeddings + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(text_embeddings)\n",
    "        fused_embeddings = self.norm2(text_embeddings + self.dropout(ffn_output))\n",
    "\n",
    "        return fused_embeddings.permute(1, 0, 2)  # [batch_size, seq_len, gpt_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLMAttn(nn.Module):\n",
    "    def __init__(self, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vit = vit_b_32(weights=ViT_B_32_Weights.IMAGENET1K_V1)\n",
    "        self.vit.heads = nn.Identity()\n",
    "\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.adapter = AttentionAdapter(\n",
    "            vit_dim=self.vit.hidden_dim,\n",
    "            gpt_dim=self.gpt2.config.n_embd,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "            )\n",
    "\n",
    "    def forward(self, images, input_ids):\n",
    "        visual_embeddings = self.vit(images).unsqueeze(1)  \n",
    "        text_embeddings = self.gpt2.transformer.wte(input_ids) \n",
    "\n",
    "        fused_embeddings = self.adapter(visual_embeddings, text_embeddings)\n",
    "\n",
    "        outputs = self.gpt2(inputs_embeds=fused_embeddings)\n",
    "        return outputs\n",
    "    \n",
    "    def generate_caption(self, images, text, max_new_tokens=50, device=\"cpu\"):\n",
    "\n",
    "        visual_features = self.vit(images).unsqueeze(1)\n",
    "\n",
    "        input_ids = self.tokenizer(text, return_tensors=\"pt\").input_ids.to(device) \n",
    "        text_embeddings = self.gpt2.transformer.wte(input_ids)\n",
    "        fused_embeddings = self.adapter(visual_features, text_embeddings)\n",
    "\n",
    "        attention_mask = torch.ones(fused_embeddings.size()[:-1], dtype=torch.long, device=device)\n",
    "        outputs = self.gpt2.generate(\n",
    "            inputs_embeds=fused_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        generated_caption = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_caption\n",
    "        \n",
    "    def freeze_vit(self):\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def freeze_gpt(self):\n",
    "        for param in self.gpt2.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm_attn = VLMAttn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "\"\n",
      "\"\n",
      "\n",
      "\"\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "caption = vlm_attn.generate_caption(image, text)\n",
    "print(\"Generated caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Flickr8kDataset(\n",
    "    tokenizer=vlm_attn.tokenizer,\n",
    "    )\n",
    "optimizer = torch.optim.Adam(vlm_attn.adapter.parameters(), lr=1e-4)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "loss = nn.CrossEntropyLoss(ignore_index=vlm_attn.tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 : 100%|██████████| 5057/5057 [07:48<00:00, 10.79it/s, loss=2.89]\n",
      "Epoch 2/5 : 100%|██████████| 5057/5057 [07:45<00:00, 10.87it/s, loss=3.54]\n",
      "Epoch 3/5 : 100%|██████████| 5057/5057 [07:44<00:00, 10.90it/s, loss=3.18]\n",
      "Epoch 4/5 : 100%|██████████| 5057/5057 [07:18<00:00, 11.53it/s, loss=3.4] \n",
      "Epoch 5/5 : 100%|██████████| 5057/5057 [07:21<00:00, 11.45it/s, loss=2.85]\n"
     ]
    }
   ],
   "source": [
    "vlm_attn.freeze_gpt()\n",
    "vlm_attn.freeze_vit()\n",
    "\n",
    "train(\n",
    "    vlm_attn,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    loss,\n",
    "    num_epochs=5,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/01/ljq5wf3159543w3273r2npnr0000gn/T/ipykernel_37762/687943398.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vlm_attn.load_state_dict(torch.load('./vlm_attn.pt', map_location='cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vlm_attn.load_state_dict(torch.load('./vlm_attn.pt', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption:  with a large number of people in the area.\n",
      "The person who was in the area was taken to the hospital.\n",
      "The person who was in the area was taken to the hospital.\n",
      "The person who was in the area was taken to\n"
     ]
    }
   ],
   "source": [
    "vlm_attn.to(\"cpu\")\n",
    "caption = vlm_attn.generate_caption(image, text)\n",
    "print(\"Generated caption:\", caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обученные архитектуры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что генерирует уже объединенные ViT и GPT-2 Воспользуемся оберткой `VisionEncoderDecoderModel` для ViT + GPT-2. Здесь авторы используют `Cross Attention`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.46.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_tr(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    pixel_values = feature_extractor(images=[img], return_tensors=\"pt\").pixel_values\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(pixel_values):\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values, max_length=50)\n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: a cat sitting on a blanket on a carpet \n"
     ]
    }
   ],
   "source": [
    "pixel_values = preprocess_image_tr(image_path)\n",
    "caption = generate_caption(pixel_values)\n",
    "print(\"Generated caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itmo_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
